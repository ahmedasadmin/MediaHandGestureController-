{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hand Gesture Controller with OpenCV and MediaPipe This Python script uses OpenCV, MediaPipe, and PyAutoGUI to enable hand gesture recognition. By tracking hand landmarks, the program allows users to perform various actions, such as controlling the system volume or simulating keypresses based on the detected hand gestures. Below is a detailed explanation of the script and its functionality. Features Gesture Recognition : Detects the number of extended fingers for both hands. Performs actions like navigating slides or controlling media playback based on finger gestures. Volume Control : Adjusts system volume using right-hand gestures based on palm size and relative landmark positions. Real-Time Processing : Processes video frames in real-time using OpenCV and MediaPipe. Debounce Mechanism : Prevents repetitive gesture recognition by adding a time-based debounce mechanism. Dependencies Python Libraries : OpenCV ( cv2 ) MediaPipe PyAutoGUI ALSA Audio ( alsaaudio ) NumPy Install the dependencies using: pip install opencv-python mediapipe pyautogui numpy pyalsaaudio How It Works 1. Hand Tracking The script uses MediaPipe's Hands solution to detect hand landmarks. It identifies key points on the hand, such as fingertips and the wrist, to track movements and gestures. 2. Gesture Recognition The number of extended fingers is calculated by comparing the relative positions of the fingertip and its base joint. Thumb extension is determined based on horizontal distance. 3. Volume Control The distance between the thumb and index finger is measured. This distance is mapped to a volume percentage and adjusts the system volume using the ALSA Audio mixer. 4. Action Mapping The following actions are performed based on the number of extended fingers: Extended Fingers Action 1 Press Right 2 Press Left 3 Press Up 4 Press Down 5 Press Space Code Walkthrough Class: HandGestureController The main class encapsulates all the functionality for gesture recognition and control. Methods: __init__() Initializes MediaPipe Hands and ALSA Audio mixer. calculate_distance(pt1, pt2) Computes Euclidean distance between two points. get_extended_count_fingers(hand_landmarks) Calculates the number of extended fingers based on hand landmarks. process_frame(frame) Processes a video frame to detect hand gestures. handle_left_hand(frame, hand_landmarks, idx) Handles gestures for the left hand. Maps gestures to keypress actions. handle_right_hand(frame, hand_landmarks) Handles gestures for the right hand. Adjusts system volume based on hand movements. release_resources() Releases MediaPipe resources. How to Run Connect a webcam to your computer. Run the script: bash python hand_gesture_control.py Use gestures to perform actions: Extend one finger to simulate a Right key press. Extend two fingers to simulate a Left key press. Adjust volume by changing the distance between your thumb and index finger (right hand). Press q to quit the application. Example Output When the script runs, a window will display the webcam feed with the following annotations: Hand landmarks connected by lines. Text indicating detected gestures and volume levels. Customization Gesture Mapping : Modify the perform_gesture_action() method to change actions for specific gestures. Volume Control : Adjust the volume range in the handle_right_hand() method by modifying the np.interp mapping. Notes Ensure the environment is well-lit for accurate hand tracking. The script is tested on Ubuntu with ALSA Audio but can be adapted for other platforms. Troubleshooting Webcam Not Detected : Ensure the webcam is properly connected. Check if another application is using the webcam. Low Gesture Detection Accuracy : Improve lighting conditions. Ensure your hand is fully visible in the frame. Volume Control Not Working : Verify ALSA Audio is properly installed and configured. Future Enhancements Add support for more complex gestures. Implement a calibration mode for dynamic gesture thresholds. Extend compatibility to non-ALSA audio systems. Acknowledgements MediaPipe for hand tracking. OpenCV for video processing. PyAutoGUI for simulating keyboard input. ALSA Audio for volume control.","title":"Home"},{"location":"#hand-gesture-controller-with-opencv-and-mediapipe","text":"This Python script uses OpenCV, MediaPipe, and PyAutoGUI to enable hand gesture recognition. By tracking hand landmarks, the program allows users to perform various actions, such as controlling the system volume or simulating keypresses based on the detected hand gestures. Below is a detailed explanation of the script and its functionality.","title":"Hand Gesture Controller with OpenCV and MediaPipe"},{"location":"#features","text":"Gesture Recognition : Detects the number of extended fingers for both hands. Performs actions like navigating slides or controlling media playback based on finger gestures. Volume Control : Adjusts system volume using right-hand gestures based on palm size and relative landmark positions. Real-Time Processing : Processes video frames in real-time using OpenCV and MediaPipe. Debounce Mechanism : Prevents repetitive gesture recognition by adding a time-based debounce mechanism.","title":"Features"},{"location":"#dependencies","text":"Python Libraries : OpenCV ( cv2 ) MediaPipe PyAutoGUI ALSA Audio ( alsaaudio ) NumPy Install the dependencies using: pip install opencv-python mediapipe pyautogui numpy pyalsaaudio","title":"Dependencies"},{"location":"#how-it-works","text":"","title":"How It Works"},{"location":"#1-hand-tracking","text":"The script uses MediaPipe's Hands solution to detect hand landmarks. It identifies key points on the hand, such as fingertips and the wrist, to track movements and gestures.","title":"1. Hand Tracking"},{"location":"#2-gesture-recognition","text":"The number of extended fingers is calculated by comparing the relative positions of the fingertip and its base joint. Thumb extension is determined based on horizontal distance.","title":"2. Gesture Recognition"},{"location":"#3-volume-control","text":"The distance between the thumb and index finger is measured. This distance is mapped to a volume percentage and adjusts the system volume using the ALSA Audio mixer.","title":"3. Volume Control"},{"location":"#4-action-mapping","text":"The following actions are performed based on the number of extended fingers: Extended Fingers Action 1 Press Right 2 Press Left 3 Press Up 4 Press Down 5 Press Space","title":"4. Action Mapping"},{"location":"#code-walkthrough","text":"","title":"Code Walkthrough"},{"location":"#class-handgesturecontroller","text":"The main class encapsulates all the functionality for gesture recognition and control.","title":"Class: HandGestureController"},{"location":"#methods","text":"__init__() Initializes MediaPipe Hands and ALSA Audio mixer. calculate_distance(pt1, pt2) Computes Euclidean distance between two points. get_extended_count_fingers(hand_landmarks) Calculates the number of extended fingers based on hand landmarks. process_frame(frame) Processes a video frame to detect hand gestures. handle_left_hand(frame, hand_landmarks, idx) Handles gestures for the left hand. Maps gestures to keypress actions. handle_right_hand(frame, hand_landmarks) Handles gestures for the right hand. Adjusts system volume based on hand movements. release_resources() Releases MediaPipe resources.","title":"Methods:"},{"location":"#how-to-run","text":"Connect a webcam to your computer. Run the script: bash python hand_gesture_control.py Use gestures to perform actions: Extend one finger to simulate a Right key press. Extend two fingers to simulate a Left key press. Adjust volume by changing the distance between your thumb and index finger (right hand). Press q to quit the application.","title":"How to Run"},{"location":"#example-output","text":"When the script runs, a window will display the webcam feed with the following annotations: Hand landmarks connected by lines. Text indicating detected gestures and volume levels.","title":"Example Output"},{"location":"#customization","text":"Gesture Mapping : Modify the perform_gesture_action() method to change actions for specific gestures. Volume Control : Adjust the volume range in the handle_right_hand() method by modifying the np.interp mapping.","title":"Customization"},{"location":"#notes","text":"Ensure the environment is well-lit for accurate hand tracking. The script is tested on Ubuntu with ALSA Audio but can be adapted for other platforms.","title":"Notes"},{"location":"#troubleshooting","text":"Webcam Not Detected : Ensure the webcam is properly connected. Check if another application is using the webcam. Low Gesture Detection Accuracy : Improve lighting conditions. Ensure your hand is fully visible in the frame. Volume Control Not Working : Verify ALSA Audio is properly installed and configured.","title":"Troubleshooting"},{"location":"#future-enhancements","text":"Add support for more complex gestures. Implement a calibration mode for dynamic gesture thresholds. Extend compatibility to non-ALSA audio systems.","title":"Future Enhancements"},{"location":"#acknowledgements","text":"MediaPipe for hand tracking. OpenCV for video processing. PyAutoGUI for simulating keyboard input. ALSA Audio for volume control.","title":"Acknowledgements"},{"location":"hand_gesture_controller/","text":"HandGestureController Source code in HGCtrl.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 class HandGestureController : def __init__ ( self ): self . mixer = alsaaudio . Mixer () self . mp_hands = mp . solutions . hands self . hands = self . mp_hands . Hands ( max_num_hands = 2 , min_detection_confidence = 0.5 , min_tracking_confidence = 0.5 ) self . mp_drawing = mp . solutions . drawing_utils self . prev_gesture = - 1 # Previous gesture to avoid repeats self . start_time = None # Timer for gesture debounce self . volume_debounce_time = time . time () # Debounce timer for volume control @staticmethod def calculate_distance ( pt1 , pt2 ): \"\"\" Calculate the Euclidean distance between two points. Args: pt1: A point object with attributes 'x' and 'y' representing the first point. pt2: A point object with attributes 'x' and 'y' representing the second point. Returns: distance(float): The Euclidean distance between the two points. \"\"\" dx = pt1 . x - pt2 . x dy = pt1 . y - pt2 . y return math . sqrt ( dx ** 2 + dy ** 2 ) def get_extended_count_fingers ( self , hand_landmarks ): \"\"\" Count the number of extended fingers based on hand landmarks. This function checks the extension of each finger using the hand landmarks, which are typically detected by a hand-tracking model like MediaPipe. It calculates the number of extended fingers by comparing the relative positions of finger tips and their corresponding bases. Additionally, it checks whether the thumb is extended based on a horizontal distance threshold. Args: hand_landmarks (object): An object containing hand landmark points, with each landmark having x, y, and z coordinates. The key landmarks used are: - landmark[0] (Wrist) - landmark[4], landmark[5] (Thumb base and joint) - landmark[6], landmark[8], landmark[10], landmark[12], landmark[14], landmark[16], landmark[18], landmark[20] (Base, joints, and tips of fingers). Returns: int: The number of extended fingers (including thumb). Logic: 1. Palm Length Calculation: - Measures the distance between the wrist (landmark[0]) and middle of the palm (landmark[9]) to calculate palm length. 2. Finger Extension Check: - Compares the y-coordinate of the finger tips with the y-coordinate of the corresponding base landmarks. - If the tip\u2019s y-coordinate is smaller than the base\u2019s, the finger is considered extended. 3. Thumb Extension Check: - Compares the horizontal distance between the thumb base (landmark[4]) and thumb joint (landmark[5]) to half the palm length. - If this distance is greater than half the palm length, the thumb is considered extended. Example: # Assuming hand_landmarks is an object containing the landmarks of the hand num_extended_fingers = get_extended_count_fingers(hand_landmarks) print(f\"Number of extended fingers: {num_extended_fingers}\") \"\"\" cnt = 0 # Calculate the palm length for thumb gesture threshold palm_length = self . calculate_distance ( hand_landmarks . landmark [ 0 ], hand_landmarks . landmark [ 9 ] ) # Check extended fingers (Index to Pinky) by comparing y-coordinates for tip , base in zip ([ 8 , 12 , 16 , 20 ], [ 6 , 10 , 14 , 18 ]): if hand_landmarks . landmark [ tip ] . y < hand_landmarks . landmark [ base ] . y : cnt += 1 # Check if Thumb is extended (using x-coordinate comparison) if self . calculate_distance ( hand_landmarks . landmark [ 4 ], hand_landmarks . landmark [ 5 ]) > palm_length * 0.5 : cnt += 1 return cnt @staticmethod def draw_landmark_circle ( frame , landmark , radius = 5 , color = ( 255 , 255 , 255 )): \"\"\" Draws a circle on a given frame at the specified landmark position. Args: frame: The image frame on which the circle will be drawn. landmark: The landmark object containing x and y coordinates. radius: The radius of the circle (default is 5). color: The color of the circle in BGR format (default is white). Returns: tuple: (x, y) coordinates of the landmark on the frame. \"\"\" x , y = int ( landmark . x * frame . shape [ 1 ]), int ( landmark . y * frame . shape [ 0 ]) cv2 . circle ( frame , ( x , y ), radius , color , - 1 ) return x , y @staticmethod def draw_line_on_frame ( frame , start , end , color = ( 255 , 255 , 255 ), thickness = 2 ): \"\"\" Draws a line on the given frame. Args: frame: The image/frame to draw on. start: A tuple (x1, y1) representing the starting point of the line. end: A tuple (x2, y2) representing the ending point of the line. color: A tuple (B, G, R) for the line color (default is white). thickness: The thickness of the line (default is 2). Returns: The updated frame with the drawn line. \"\"\" cv2 . line ( frame , start , end , color , thickness ) return frame def process_frame ( self , frame ): \"\"\" Process a single video frame to detect hands and gestures. Args: frame: The input video frame (BGR format). Returns: The processed frame with drawn landmarks and annotations. \"\"\" rgb_frame = cv2 . cvtColor ( frame , cv2 . COLOR_BGR2RGB ) results = self . hands . process ( rgb_frame ) if results . multi_hand_landmarks : for idx , hand_landmarks in enumerate ( results . multi_hand_landmarks ): handedness = results . multi_handedness [ idx ] . classification [ 0 ] . label if handedness == \"Left\" : self . handle_left_hand ( frame , hand_landmarks , idx ) elif handedness == \"Right\" : self . handle_right_hand ( frame , hand_landmarks ) # Draw Landmarks self . mp_drawing . draw_landmarks ( frame , hand_landmarks , self . mp_hands . HAND_CONNECTIONS ) return frame def handle_left_hand ( self , frame , hand_landmarks , idx ): \"\"\" Handle gestures detected from the left hand. Args: frame: The video frame to annotate. hand_landmarks: The landmarks of the detected left hand. idx: The index of the hand. \"\"\" finger_count = self . get_extended_count_fingers ( hand_landmarks ) label = f \"Left Hand: { finger_count } Fingers\" cv2 . putText ( frame , label , ( 10 , 50 + idx * 30 ), cv2 . FONT_HERSHEY_SIMPLEX , 1 , ( 255 , 0 , 0 ), 2 ) current_time = time . time () if self . prev_gesture != finger_count : if self . start_time is None : self . start_time = current_time elif current_time - self . start_time > 0.3 : # Debounce threshold self . perform_gesture_action ( finger_count ) self . prev_gesture = finger_count self . start_time = None @staticmethod def perform_gesture_action ( finger_count ): \"\"\" Perform actions based on the detected gesture. Args: finger_count: The number of extended fingers detected. \"\"\" if finger_count == 1 : pyautogui . press ( \"right\" ) elif finger_count == 2 : pyautogui . press ( \"left\" ) elif finger_count == 3 : pyautogui . press ( \"up\" ) elif finger_count == 4 : pyautogui . press ( \"down\" ) elif finger_count == 5 : pyautogui . press ( \"space\" ) def handle_right_hand ( self , frame , hand_landmarks ): \"\"\" Handle volume control gestures detected from the right hand. Args: frame: The video frame to annotate. hand_landmarks: The landmarks of the detected right hand. \"\"\" palm_length = self . calculate_distance ( hand_landmarks . landmark [ 0 ], hand_landmarks . landmark [ 9 ]) hand_width = self . calculate_distance ( hand_landmarks . landmark [ 8 ], hand_landmarks . landmark [ 4 ]) palm_ratio = hand_width / palm_length if hand_width != 0 else 0 x1 , y1 = self . draw_landmark_circle ( frame , hand_landmarks . landmark [ 8 ]) x2 , y2 = self . draw_landmark_circle ( frame , hand_landmarks . landmark [ 4 ]) self . draw_line_on_frame ( frame , ( x1 , y1 ), ( x2 , y2 ), color = ( 0 , 0 , 0 )) volume = int ( np . interp ( palm_ratio , [ 0.2 , 1.50 ], [ 0 , 100 ])) current_time = time . time () if current_time - self . volume_debounce_time > 0.3 : # 300ms debounce self . mixer . setvolume ( volume ) self . volume_debounce_time = current_time cv2 . putText ( frame , f \"Volume: { volume } %\" , ( 10 , 100 ), cv2 . FONT_HERSHEY_SIMPLEX , 1 , ( 0 , 255 , 0 ), 2 ) def release_resources ( self ): \"\"\" Release resources used by the hand tracking model. \"\"\" self . hands . close () calculate_distance ( pt1 , pt2 ) staticmethod Calculate the Euclidean distance between two points. Parameters: pt1 \u2013 A point object with attributes 'x' and 'y' representing the first point. pt2 \u2013 A point object with attributes 'x' and 'y' representing the second point. Returns: distance ( float ) \u2013 The Euclidean distance between the two points. Source code in HGCtrl.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @staticmethod def calculate_distance ( pt1 , pt2 ): \"\"\" Calculate the Euclidean distance between two points. Args: pt1: A point object with attributes 'x' and 'y' representing the first point. pt2: A point object with attributes 'x' and 'y' representing the second point. Returns: distance(float): The Euclidean distance between the two points. \"\"\" dx = pt1 . x - pt2 . x dy = pt1 . y - pt2 . y return math . sqrt ( dx ** 2 + dy ** 2 ) draw_landmark_circle ( frame , landmark , radius = 5 , color = ( 255 , 255 , 255 )) staticmethod Draws a circle on a given frame at the specified landmark position. Parameters: frame \u2013 The image frame on which the circle will be drawn. landmark \u2013 The landmark object containing x and y coordinates. radius \u2013 The radius of the circle (default is 5). color \u2013 The color of the circle in BGR format (default is white). Returns: tuple \u2013 (x, y) coordinates of the landmark on the frame. Source code in HGCtrl.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 @staticmethod def draw_landmark_circle ( frame , landmark , radius = 5 , color = ( 255 , 255 , 255 )): \"\"\" Draws a circle on a given frame at the specified landmark position. Args: frame: The image frame on which the circle will be drawn. landmark: The landmark object containing x and y coordinates. radius: The radius of the circle (default is 5). color: The color of the circle in BGR format (default is white). Returns: tuple: (x, y) coordinates of the landmark on the frame. \"\"\" x , y = int ( landmark . x * frame . shape [ 1 ]), int ( landmark . y * frame . shape [ 0 ]) cv2 . circle ( frame , ( x , y ), radius , color , - 1 ) return x , y draw_line_on_frame ( frame , start , end , color = ( 255 , 255 , 255 ), thickness = 2 ) staticmethod Draws a line on the given frame. Parameters: frame \u2013 The image/frame to draw on. start \u2013 A tuple (x1, y1) representing the starting point of the line. end \u2013 A tuple (x2, y2) representing the ending point of the line. color \u2013 A tuple (B, G, R) for the line color (default is white). thickness \u2013 The thickness of the line (default is 2). Returns: \u2013 The updated frame with the drawn line. Source code in HGCtrl.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 @staticmethod def draw_line_on_frame ( frame , start , end , color = ( 255 , 255 , 255 ), thickness = 2 ): \"\"\" Draws a line on the given frame. Args: frame: The image/frame to draw on. start: A tuple (x1, y1) representing the starting point of the line. end: A tuple (x2, y2) representing the ending point of the line. color: A tuple (B, G, R) for the line color (default is white). thickness: The thickness of the line (default is 2). Returns: The updated frame with the drawn line. \"\"\" cv2 . line ( frame , start , end , color , thickness ) return frame get_extended_count_fingers ( hand_landmarks ) Count the number of extended fingers based on hand landmarks. This function checks the extension of each finger using the hand landmarks, which are typically detected by a hand-tracking model like MediaPipe. It calculates the number of extended fingers by comparing the relative positions of finger tips and their corresponding bases. Additionally, it checks whether the thumb is extended based on a horizontal distance threshold. Parameters: hand_landmarks ( object ) \u2013 An object containing hand landmark points, with each landmark ( having x, y, and z coordinates. The key landmarks used are ) \u2013 landmark[0] (Wrist) landmark[4], landmark[5] (Thumb base and joint) landmark[6], landmark[8], landmark[10], landmark[12], landmark[14], landmark[16], landmark[18], landmark[20] (Base, joints, and tips of fingers). Returns: int \u2013 The number of extended fingers (including thumb). Logic Palm Length Calculation: Measures the distance between the wrist (landmark[0]) and middle of the palm (landmark[9]) to calculate palm length. Finger Extension Check: Compares the y-coordinate of the finger tips with the y-coordinate of the corresponding base landmarks. If the tip\u2019s y-coordinate is smaller than the base\u2019s, the finger is considered extended. Thumb Extension Check: Compares the horizontal distance between the thumb base (landmark[4]) and thumb joint (landmark[5]) to half the palm length. If this distance is greater than half the palm length, the thumb is considered extended. Example Assuming hand_landmarks is an object containing the landmarks of the hand num_extended_fingers = get_extended_count_fingers(hand_landmarks) print(f\"Number of extended fingers: {num_extended_fingers}\") Source code in HGCtrl.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def get_extended_count_fingers ( self , hand_landmarks ): \"\"\" Count the number of extended fingers based on hand landmarks. This function checks the extension of each finger using the hand landmarks, which are typically detected by a hand-tracking model like MediaPipe. It calculates the number of extended fingers by comparing the relative positions of finger tips and their corresponding bases. Additionally, it checks whether the thumb is extended based on a horizontal distance threshold. Args: hand_landmarks (object): An object containing hand landmark points, with each landmark having x, y, and z coordinates. The key landmarks used are: - landmark[0] (Wrist) - landmark[4], landmark[5] (Thumb base and joint) - landmark[6], landmark[8], landmark[10], landmark[12], landmark[14], landmark[16], landmark[18], landmark[20] (Base, joints, and tips of fingers). Returns: int: The number of extended fingers (including thumb). Logic: 1. Palm Length Calculation: - Measures the distance between the wrist (landmark[0]) and middle of the palm (landmark[9]) to calculate palm length. 2. Finger Extension Check: - Compares the y-coordinate of the finger tips with the y-coordinate of the corresponding base landmarks. - If the tip\u2019s y-coordinate is smaller than the base\u2019s, the finger is considered extended. 3. Thumb Extension Check: - Compares the horizontal distance between the thumb base (landmark[4]) and thumb joint (landmark[5]) to half the palm length. - If this distance is greater than half the palm length, the thumb is considered extended. Example: # Assuming hand_landmarks is an object containing the landmarks of the hand num_extended_fingers = get_extended_count_fingers(hand_landmarks) print(f\"Number of extended fingers: {num_extended_fingers}\") \"\"\" cnt = 0 # Calculate the palm length for thumb gesture threshold palm_length = self . calculate_distance ( hand_landmarks . landmark [ 0 ], hand_landmarks . landmark [ 9 ] ) # Check extended fingers (Index to Pinky) by comparing y-coordinates for tip , base in zip ([ 8 , 12 , 16 , 20 ], [ 6 , 10 , 14 , 18 ]): if hand_landmarks . landmark [ tip ] . y < hand_landmarks . landmark [ base ] . y : cnt += 1 # Check if Thumb is extended (using x-coordinate comparison) if self . calculate_distance ( hand_landmarks . landmark [ 4 ], hand_landmarks . landmark [ 5 ]) > palm_length * 0.5 : cnt += 1 return cnt handle_left_hand ( frame , hand_landmarks , idx ) Handle gestures detected from the left hand. Parameters: frame \u2013 The video frame to annotate. hand_landmarks \u2013 The landmarks of the detected left hand. idx \u2013 The index of the hand. Source code in HGCtrl.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def handle_left_hand ( self , frame , hand_landmarks , idx ): \"\"\" Handle gestures detected from the left hand. Args: frame: The video frame to annotate. hand_landmarks: The landmarks of the detected left hand. idx: The index of the hand. \"\"\" finger_count = self . get_extended_count_fingers ( hand_landmarks ) label = f \"Left Hand: { finger_count } Fingers\" cv2 . putText ( frame , label , ( 10 , 50 + idx * 30 ), cv2 . FONT_HERSHEY_SIMPLEX , 1 , ( 255 , 0 , 0 ), 2 ) current_time = time . time () if self . prev_gesture != finger_count : if self . start_time is None : self . start_time = current_time elif current_time - self . start_time > 0.3 : # Debounce threshold self . perform_gesture_action ( finger_count ) self . prev_gesture = finger_count self . start_time = None handle_right_hand ( frame , hand_landmarks ) Handle volume control gestures detected from the right hand. Parameters: frame \u2013 The video frame to annotate. hand_landmarks \u2013 The landmarks of the detected right hand. Source code in HGCtrl.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def handle_right_hand ( self , frame , hand_landmarks ): \"\"\" Handle volume control gestures detected from the right hand. Args: frame: The video frame to annotate. hand_landmarks: The landmarks of the detected right hand. \"\"\" palm_length = self . calculate_distance ( hand_landmarks . landmark [ 0 ], hand_landmarks . landmark [ 9 ]) hand_width = self . calculate_distance ( hand_landmarks . landmark [ 8 ], hand_landmarks . landmark [ 4 ]) palm_ratio = hand_width / palm_length if hand_width != 0 else 0 x1 , y1 = self . draw_landmark_circle ( frame , hand_landmarks . landmark [ 8 ]) x2 , y2 = self . draw_landmark_circle ( frame , hand_landmarks . landmark [ 4 ]) self . draw_line_on_frame ( frame , ( x1 , y1 ), ( x2 , y2 ), color = ( 0 , 0 , 0 )) volume = int ( np . interp ( palm_ratio , [ 0.2 , 1.50 ], [ 0 , 100 ])) current_time = time . time () if current_time - self . volume_debounce_time > 0.3 : # 300ms debounce self . mixer . setvolume ( volume ) self . volume_debounce_time = current_time cv2 . putText ( frame , f \"Volume: { volume } %\" , ( 10 , 100 ), cv2 . FONT_HERSHEY_SIMPLEX , 1 , ( 0 , 255 , 0 ), 2 ) perform_gesture_action ( finger_count ) staticmethod Perform actions based on the detected gesture. Parameters: finger_count \u2013 The number of extended fingers detected. Source code in HGCtrl.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 @staticmethod def perform_gesture_action ( finger_count ): \"\"\" Perform actions based on the detected gesture. Args: finger_count: The number of extended fingers detected. \"\"\" if finger_count == 1 : pyautogui . press ( \"right\" ) elif finger_count == 2 : pyautogui . press ( \"left\" ) elif finger_count == 3 : pyautogui . press ( \"up\" ) elif finger_count == 4 : pyautogui . press ( \"down\" ) elif finger_count == 5 : pyautogui . press ( \"space\" ) process_frame ( frame ) Process a single video frame to detect hands and gestures. Parameters: frame \u2013 The input video frame (BGR format). Returns: \u2013 The processed frame with drawn landmarks and annotations. Source code in HGCtrl.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def process_frame ( self , frame ): \"\"\" Process a single video frame to detect hands and gestures. Args: frame: The input video frame (BGR format). Returns: The processed frame with drawn landmarks and annotations. \"\"\" rgb_frame = cv2 . cvtColor ( frame , cv2 . COLOR_BGR2RGB ) results = self . hands . process ( rgb_frame ) if results . multi_hand_landmarks : for idx , hand_landmarks in enumerate ( results . multi_hand_landmarks ): handedness = results . multi_handedness [ idx ] . classification [ 0 ] . label if handedness == \"Left\" : self . handle_left_hand ( frame , hand_landmarks , idx ) elif handedness == \"Right\" : self . handle_right_hand ( frame , hand_landmarks ) # Draw Landmarks self . mp_drawing . draw_landmarks ( frame , hand_landmarks , self . mp_hands . HAND_CONNECTIONS ) return frame release_resources () Release resources used by the hand tracking model. Source code in HGCtrl.py 224 225 226 227 228 def release_resources ( self ): \"\"\" Release resources used by the hand tracking model. \"\"\" self . hands . close ()","title":"HandGestureController"},{"location":"hand_gesture_controller/#handgesturecontroller","text":"Source code in HGCtrl.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 class HandGestureController : def __init__ ( self ): self . mixer = alsaaudio . Mixer () self . mp_hands = mp . solutions . hands self . hands = self . mp_hands . Hands ( max_num_hands = 2 , min_detection_confidence = 0.5 , min_tracking_confidence = 0.5 ) self . mp_drawing = mp . solutions . drawing_utils self . prev_gesture = - 1 # Previous gesture to avoid repeats self . start_time = None # Timer for gesture debounce self . volume_debounce_time = time . time () # Debounce timer for volume control @staticmethod def calculate_distance ( pt1 , pt2 ): \"\"\" Calculate the Euclidean distance between two points. Args: pt1: A point object with attributes 'x' and 'y' representing the first point. pt2: A point object with attributes 'x' and 'y' representing the second point. Returns: distance(float): The Euclidean distance between the two points. \"\"\" dx = pt1 . x - pt2 . x dy = pt1 . y - pt2 . y return math . sqrt ( dx ** 2 + dy ** 2 ) def get_extended_count_fingers ( self , hand_landmarks ): \"\"\" Count the number of extended fingers based on hand landmarks. This function checks the extension of each finger using the hand landmarks, which are typically detected by a hand-tracking model like MediaPipe. It calculates the number of extended fingers by comparing the relative positions of finger tips and their corresponding bases. Additionally, it checks whether the thumb is extended based on a horizontal distance threshold. Args: hand_landmarks (object): An object containing hand landmark points, with each landmark having x, y, and z coordinates. The key landmarks used are: - landmark[0] (Wrist) - landmark[4], landmark[5] (Thumb base and joint) - landmark[6], landmark[8], landmark[10], landmark[12], landmark[14], landmark[16], landmark[18], landmark[20] (Base, joints, and tips of fingers). Returns: int: The number of extended fingers (including thumb). Logic: 1. Palm Length Calculation: - Measures the distance between the wrist (landmark[0]) and middle of the palm (landmark[9]) to calculate palm length. 2. Finger Extension Check: - Compares the y-coordinate of the finger tips with the y-coordinate of the corresponding base landmarks. - If the tip\u2019s y-coordinate is smaller than the base\u2019s, the finger is considered extended. 3. Thumb Extension Check: - Compares the horizontal distance between the thumb base (landmark[4]) and thumb joint (landmark[5]) to half the palm length. - If this distance is greater than half the palm length, the thumb is considered extended. Example: # Assuming hand_landmarks is an object containing the landmarks of the hand num_extended_fingers = get_extended_count_fingers(hand_landmarks) print(f\"Number of extended fingers: {num_extended_fingers}\") \"\"\" cnt = 0 # Calculate the palm length for thumb gesture threshold palm_length = self . calculate_distance ( hand_landmarks . landmark [ 0 ], hand_landmarks . landmark [ 9 ] ) # Check extended fingers (Index to Pinky) by comparing y-coordinates for tip , base in zip ([ 8 , 12 , 16 , 20 ], [ 6 , 10 , 14 , 18 ]): if hand_landmarks . landmark [ tip ] . y < hand_landmarks . landmark [ base ] . y : cnt += 1 # Check if Thumb is extended (using x-coordinate comparison) if self . calculate_distance ( hand_landmarks . landmark [ 4 ], hand_landmarks . landmark [ 5 ]) > palm_length * 0.5 : cnt += 1 return cnt @staticmethod def draw_landmark_circle ( frame , landmark , radius = 5 , color = ( 255 , 255 , 255 )): \"\"\" Draws a circle on a given frame at the specified landmark position. Args: frame: The image frame on which the circle will be drawn. landmark: The landmark object containing x and y coordinates. radius: The radius of the circle (default is 5). color: The color of the circle in BGR format (default is white). Returns: tuple: (x, y) coordinates of the landmark on the frame. \"\"\" x , y = int ( landmark . x * frame . shape [ 1 ]), int ( landmark . y * frame . shape [ 0 ]) cv2 . circle ( frame , ( x , y ), radius , color , - 1 ) return x , y @staticmethod def draw_line_on_frame ( frame , start , end , color = ( 255 , 255 , 255 ), thickness = 2 ): \"\"\" Draws a line on the given frame. Args: frame: The image/frame to draw on. start: A tuple (x1, y1) representing the starting point of the line. end: A tuple (x2, y2) representing the ending point of the line. color: A tuple (B, G, R) for the line color (default is white). thickness: The thickness of the line (default is 2). Returns: The updated frame with the drawn line. \"\"\" cv2 . line ( frame , start , end , color , thickness ) return frame def process_frame ( self , frame ): \"\"\" Process a single video frame to detect hands and gestures. Args: frame: The input video frame (BGR format). Returns: The processed frame with drawn landmarks and annotations. \"\"\" rgb_frame = cv2 . cvtColor ( frame , cv2 . COLOR_BGR2RGB ) results = self . hands . process ( rgb_frame ) if results . multi_hand_landmarks : for idx , hand_landmarks in enumerate ( results . multi_hand_landmarks ): handedness = results . multi_handedness [ idx ] . classification [ 0 ] . label if handedness == \"Left\" : self . handle_left_hand ( frame , hand_landmarks , idx ) elif handedness == \"Right\" : self . handle_right_hand ( frame , hand_landmarks ) # Draw Landmarks self . mp_drawing . draw_landmarks ( frame , hand_landmarks , self . mp_hands . HAND_CONNECTIONS ) return frame def handle_left_hand ( self , frame , hand_landmarks , idx ): \"\"\" Handle gestures detected from the left hand. Args: frame: The video frame to annotate. hand_landmarks: The landmarks of the detected left hand. idx: The index of the hand. \"\"\" finger_count = self . get_extended_count_fingers ( hand_landmarks ) label = f \"Left Hand: { finger_count } Fingers\" cv2 . putText ( frame , label , ( 10 , 50 + idx * 30 ), cv2 . FONT_HERSHEY_SIMPLEX , 1 , ( 255 , 0 , 0 ), 2 ) current_time = time . time () if self . prev_gesture != finger_count : if self . start_time is None : self . start_time = current_time elif current_time - self . start_time > 0.3 : # Debounce threshold self . perform_gesture_action ( finger_count ) self . prev_gesture = finger_count self . start_time = None @staticmethod def perform_gesture_action ( finger_count ): \"\"\" Perform actions based on the detected gesture. Args: finger_count: The number of extended fingers detected. \"\"\" if finger_count == 1 : pyautogui . press ( \"right\" ) elif finger_count == 2 : pyautogui . press ( \"left\" ) elif finger_count == 3 : pyautogui . press ( \"up\" ) elif finger_count == 4 : pyautogui . press ( \"down\" ) elif finger_count == 5 : pyautogui . press ( \"space\" ) def handle_right_hand ( self , frame , hand_landmarks ): \"\"\" Handle volume control gestures detected from the right hand. Args: frame: The video frame to annotate. hand_landmarks: The landmarks of the detected right hand. \"\"\" palm_length = self . calculate_distance ( hand_landmarks . landmark [ 0 ], hand_landmarks . landmark [ 9 ]) hand_width = self . calculate_distance ( hand_landmarks . landmark [ 8 ], hand_landmarks . landmark [ 4 ]) palm_ratio = hand_width / palm_length if hand_width != 0 else 0 x1 , y1 = self . draw_landmark_circle ( frame , hand_landmarks . landmark [ 8 ]) x2 , y2 = self . draw_landmark_circle ( frame , hand_landmarks . landmark [ 4 ]) self . draw_line_on_frame ( frame , ( x1 , y1 ), ( x2 , y2 ), color = ( 0 , 0 , 0 )) volume = int ( np . interp ( palm_ratio , [ 0.2 , 1.50 ], [ 0 , 100 ])) current_time = time . time () if current_time - self . volume_debounce_time > 0.3 : # 300ms debounce self . mixer . setvolume ( volume ) self . volume_debounce_time = current_time cv2 . putText ( frame , f \"Volume: { volume } %\" , ( 10 , 100 ), cv2 . FONT_HERSHEY_SIMPLEX , 1 , ( 0 , 255 , 0 ), 2 ) def release_resources ( self ): \"\"\" Release resources used by the hand tracking model. \"\"\" self . hands . close ()","title":"HandGestureController"},{"location":"hand_gesture_controller/#HGCtrl.HandGestureController.calculate_distance","text":"Calculate the Euclidean distance between two points. Parameters: pt1 \u2013 A point object with attributes 'x' and 'y' representing the first point. pt2 \u2013 A point object with attributes 'x' and 'y' representing the second point. Returns: distance ( float ) \u2013 The Euclidean distance between the two points. Source code in HGCtrl.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @staticmethod def calculate_distance ( pt1 , pt2 ): \"\"\" Calculate the Euclidean distance between two points. Args: pt1: A point object with attributes 'x' and 'y' representing the first point. pt2: A point object with attributes 'x' and 'y' representing the second point. Returns: distance(float): The Euclidean distance between the two points. \"\"\" dx = pt1 . x - pt2 . x dy = pt1 . y - pt2 . y return math . sqrt ( dx ** 2 + dy ** 2 )","title":"calculate_distance()"},{"location":"hand_gesture_controller/#HGCtrl.HandGestureController.draw_landmark_circle","text":"Draws a circle on a given frame at the specified landmark position. Parameters: frame \u2013 The image frame on which the circle will be drawn. landmark \u2013 The landmark object containing x and y coordinates. radius \u2013 The radius of the circle (default is 5). color \u2013 The color of the circle in BGR format (default is white). Returns: tuple \u2013 (x, y) coordinates of the landmark on the frame. Source code in HGCtrl.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 @staticmethod def draw_landmark_circle ( frame , landmark , radius = 5 , color = ( 255 , 255 , 255 )): \"\"\" Draws a circle on a given frame at the specified landmark position. Args: frame: The image frame on which the circle will be drawn. landmark: The landmark object containing x and y coordinates. radius: The radius of the circle (default is 5). color: The color of the circle in BGR format (default is white). Returns: tuple: (x, y) coordinates of the landmark on the frame. \"\"\" x , y = int ( landmark . x * frame . shape [ 1 ]), int ( landmark . y * frame . shape [ 0 ]) cv2 . circle ( frame , ( x , y ), radius , color , - 1 ) return x , y","title":"draw_landmark_circle()"},{"location":"hand_gesture_controller/#HGCtrl.HandGestureController.draw_line_on_frame","text":"Draws a line on the given frame. Parameters: frame \u2013 The image/frame to draw on. start \u2013 A tuple (x1, y1) representing the starting point of the line. end \u2013 A tuple (x2, y2) representing the ending point of the line. color \u2013 A tuple (B, G, R) for the line color (default is white). thickness \u2013 The thickness of the line (default is 2). Returns: \u2013 The updated frame with the drawn line. Source code in HGCtrl.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 @staticmethod def draw_line_on_frame ( frame , start , end , color = ( 255 , 255 , 255 ), thickness = 2 ): \"\"\" Draws a line on the given frame. Args: frame: The image/frame to draw on. start: A tuple (x1, y1) representing the starting point of the line. end: A tuple (x2, y2) representing the ending point of the line. color: A tuple (B, G, R) for the line color (default is white). thickness: The thickness of the line (default is 2). Returns: The updated frame with the drawn line. \"\"\" cv2 . line ( frame , start , end , color , thickness ) return frame","title":"draw_line_on_frame()"},{"location":"hand_gesture_controller/#HGCtrl.HandGestureController.get_extended_count_fingers","text":"Count the number of extended fingers based on hand landmarks. This function checks the extension of each finger using the hand landmarks, which are typically detected by a hand-tracking model like MediaPipe. It calculates the number of extended fingers by comparing the relative positions of finger tips and their corresponding bases. Additionally, it checks whether the thumb is extended based on a horizontal distance threshold. Parameters: hand_landmarks ( object ) \u2013 An object containing hand landmark points, with each landmark ( having x, y, and z coordinates. The key landmarks used are ) \u2013 landmark[0] (Wrist) landmark[4], landmark[5] (Thumb base and joint) landmark[6], landmark[8], landmark[10], landmark[12], landmark[14], landmark[16], landmark[18], landmark[20] (Base, joints, and tips of fingers). Returns: int \u2013 The number of extended fingers (including thumb). Logic Palm Length Calculation: Measures the distance between the wrist (landmark[0]) and middle of the palm (landmark[9]) to calculate palm length. Finger Extension Check: Compares the y-coordinate of the finger tips with the y-coordinate of the corresponding base landmarks. If the tip\u2019s y-coordinate is smaller than the base\u2019s, the finger is considered extended. Thumb Extension Check: Compares the horizontal distance between the thumb base (landmark[4]) and thumb joint (landmark[5]) to half the palm length. If this distance is greater than half the palm length, the thumb is considered extended. Example","title":"get_extended_count_fingers()"},{"location":"hand_gesture_controller/#HGCtrl.HandGestureController.get_extended_count_fingers--assuming-hand_landmarks-is-an-object-containing-the-landmarks-of-the-hand","text":"num_extended_fingers = get_extended_count_fingers(hand_landmarks) print(f\"Number of extended fingers: {num_extended_fingers}\") Source code in HGCtrl.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def get_extended_count_fingers ( self , hand_landmarks ): \"\"\" Count the number of extended fingers based on hand landmarks. This function checks the extension of each finger using the hand landmarks, which are typically detected by a hand-tracking model like MediaPipe. It calculates the number of extended fingers by comparing the relative positions of finger tips and their corresponding bases. Additionally, it checks whether the thumb is extended based on a horizontal distance threshold. Args: hand_landmarks (object): An object containing hand landmark points, with each landmark having x, y, and z coordinates. The key landmarks used are: - landmark[0] (Wrist) - landmark[4], landmark[5] (Thumb base and joint) - landmark[6], landmark[8], landmark[10], landmark[12], landmark[14], landmark[16], landmark[18], landmark[20] (Base, joints, and tips of fingers). Returns: int: The number of extended fingers (including thumb). Logic: 1. Palm Length Calculation: - Measures the distance between the wrist (landmark[0]) and middle of the palm (landmark[9]) to calculate palm length. 2. Finger Extension Check: - Compares the y-coordinate of the finger tips with the y-coordinate of the corresponding base landmarks. - If the tip\u2019s y-coordinate is smaller than the base\u2019s, the finger is considered extended. 3. Thumb Extension Check: - Compares the horizontal distance between the thumb base (landmark[4]) and thumb joint (landmark[5]) to half the palm length. - If this distance is greater than half the palm length, the thumb is considered extended. Example: # Assuming hand_landmarks is an object containing the landmarks of the hand num_extended_fingers = get_extended_count_fingers(hand_landmarks) print(f\"Number of extended fingers: {num_extended_fingers}\") \"\"\" cnt = 0 # Calculate the palm length for thumb gesture threshold palm_length = self . calculate_distance ( hand_landmarks . landmark [ 0 ], hand_landmarks . landmark [ 9 ] ) # Check extended fingers (Index to Pinky) by comparing y-coordinates for tip , base in zip ([ 8 , 12 , 16 , 20 ], [ 6 , 10 , 14 , 18 ]): if hand_landmarks . landmark [ tip ] . y < hand_landmarks . landmark [ base ] . y : cnt += 1 # Check if Thumb is extended (using x-coordinate comparison) if self . calculate_distance ( hand_landmarks . landmark [ 4 ], hand_landmarks . landmark [ 5 ]) > palm_length * 0.5 : cnt += 1 return cnt","title":"Assuming hand_landmarks is an object containing the landmarks of the hand"},{"location":"hand_gesture_controller/#HGCtrl.HandGestureController.handle_left_hand","text":"Handle gestures detected from the left hand. Parameters: frame \u2013 The video frame to annotate. hand_landmarks \u2013 The landmarks of the detected left hand. idx \u2013 The index of the hand. Source code in HGCtrl.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def handle_left_hand ( self , frame , hand_landmarks , idx ): \"\"\" Handle gestures detected from the left hand. Args: frame: The video frame to annotate. hand_landmarks: The landmarks of the detected left hand. idx: The index of the hand. \"\"\" finger_count = self . get_extended_count_fingers ( hand_landmarks ) label = f \"Left Hand: { finger_count } Fingers\" cv2 . putText ( frame , label , ( 10 , 50 + idx * 30 ), cv2 . FONT_HERSHEY_SIMPLEX , 1 , ( 255 , 0 , 0 ), 2 ) current_time = time . time () if self . prev_gesture != finger_count : if self . start_time is None : self . start_time = current_time elif current_time - self . start_time > 0.3 : # Debounce threshold self . perform_gesture_action ( finger_count ) self . prev_gesture = finger_count self . start_time = None","title":"handle_left_hand()"},{"location":"hand_gesture_controller/#HGCtrl.HandGestureController.handle_right_hand","text":"Handle volume control gestures detected from the right hand. Parameters: frame \u2013 The video frame to annotate. hand_landmarks \u2013 The landmarks of the detected right hand. Source code in HGCtrl.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def handle_right_hand ( self , frame , hand_landmarks ): \"\"\" Handle volume control gestures detected from the right hand. Args: frame: The video frame to annotate. hand_landmarks: The landmarks of the detected right hand. \"\"\" palm_length = self . calculate_distance ( hand_landmarks . landmark [ 0 ], hand_landmarks . landmark [ 9 ]) hand_width = self . calculate_distance ( hand_landmarks . landmark [ 8 ], hand_landmarks . landmark [ 4 ]) palm_ratio = hand_width / palm_length if hand_width != 0 else 0 x1 , y1 = self . draw_landmark_circle ( frame , hand_landmarks . landmark [ 8 ]) x2 , y2 = self . draw_landmark_circle ( frame , hand_landmarks . landmark [ 4 ]) self . draw_line_on_frame ( frame , ( x1 , y1 ), ( x2 , y2 ), color = ( 0 , 0 , 0 )) volume = int ( np . interp ( palm_ratio , [ 0.2 , 1.50 ], [ 0 , 100 ])) current_time = time . time () if current_time - self . volume_debounce_time > 0.3 : # 300ms debounce self . mixer . setvolume ( volume ) self . volume_debounce_time = current_time cv2 . putText ( frame , f \"Volume: { volume } %\" , ( 10 , 100 ), cv2 . FONT_HERSHEY_SIMPLEX , 1 , ( 0 , 255 , 0 ), 2 )","title":"handle_right_hand()"},{"location":"hand_gesture_controller/#HGCtrl.HandGestureController.perform_gesture_action","text":"Perform actions based on the detected gesture. Parameters: finger_count \u2013 The number of extended fingers detected. Source code in HGCtrl.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 @staticmethod def perform_gesture_action ( finger_count ): \"\"\" Perform actions based on the detected gesture. Args: finger_count: The number of extended fingers detected. \"\"\" if finger_count == 1 : pyautogui . press ( \"right\" ) elif finger_count == 2 : pyautogui . press ( \"left\" ) elif finger_count == 3 : pyautogui . press ( \"up\" ) elif finger_count == 4 : pyautogui . press ( \"down\" ) elif finger_count == 5 : pyautogui . press ( \"space\" )","title":"perform_gesture_action()"},{"location":"hand_gesture_controller/#HGCtrl.HandGestureController.process_frame","text":"Process a single video frame to detect hands and gestures. Parameters: frame \u2013 The input video frame (BGR format). Returns: \u2013 The processed frame with drawn landmarks and annotations. Source code in HGCtrl.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def process_frame ( self , frame ): \"\"\" Process a single video frame to detect hands and gestures. Args: frame: The input video frame (BGR format). Returns: The processed frame with drawn landmarks and annotations. \"\"\" rgb_frame = cv2 . cvtColor ( frame , cv2 . COLOR_BGR2RGB ) results = self . hands . process ( rgb_frame ) if results . multi_hand_landmarks : for idx , hand_landmarks in enumerate ( results . multi_hand_landmarks ): handedness = results . multi_handedness [ idx ] . classification [ 0 ] . label if handedness == \"Left\" : self . handle_left_hand ( frame , hand_landmarks , idx ) elif handedness == \"Right\" : self . handle_right_hand ( frame , hand_landmarks ) # Draw Landmarks self . mp_drawing . draw_landmarks ( frame , hand_landmarks , self . mp_hands . HAND_CONNECTIONS ) return frame","title":"process_frame()"},{"location":"hand_gesture_controller/#HGCtrl.HandGestureController.release_resources","text":"Release resources used by the hand tracking model. Source code in HGCtrl.py 224 225 226 227 228 def release_resources ( self ): \"\"\" Release resources used by the hand tracking model. \"\"\" self . hands . close ()","title":"release_resources()"}]}